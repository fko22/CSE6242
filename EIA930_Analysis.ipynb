{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc6ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==2.0.3\n",
      "  Using cached pandas-2.0.3-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Collecting numpy==1.24.4\n",
      "  Using cached numpy-1.24.4-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Collecting pyvis==0.3.2\n",
      "  Using cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "Collecting networkx==3.1\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting matplotlib==3.7.2\n",
      "  Downloading matplotlib-3.7.2-cp39-cp39-macosx_10_12_x86_64.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 3.7 MB/s eta 0:00:01     |█████                           | 1.1 MB 3.7 MB/s eta 0:00:02     |██████████████████████████████▉ | 7.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geopandas==0.14.4\n",
      "  Downloading geopandas-0.14.4-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fiona==1.9.6\n",
      "  Downloading fiona-1.9.6-cp39-cp39-macosx_10_15_x86_64.whl (18.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.7 MB 423 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (3.1.4)\n",
      "Collecting jsonpickle>=1.4.1\n",
      "  Using cached jsonpickle-4.0.0-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (8.12.3)\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl (265 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-macosx_10_9_x86_64.whl (65 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (24.1)\n",
      "Collecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp39-cp39-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shapely>=1.8.0\n",
      "  Downloading shapely-2.0.6-cp39-cp39-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 11.1 MB/s eta 0:00:01     |█████▉                          | 266 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyproj>=3.3.0\n",
      "  Downloading pyproj-3.6.1-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (8.5.0)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (2024.8.30)\n",
      "Requirement already satisfied: six in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (24.2.0)\n",
      "Collecting click~=8.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2>=2.9.6->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.14.3)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.48)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib==3.7.2->-r requirements.txt (line 6)) (3.20.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.3)\n",
      "Installing collected packages: numpy, pandas, jsonpickle, networkx, pyvis, pyparsing, cycler, contourpy, kiwisolver, importlib-resources, fonttools, matplotlib, click, cligj, click-plugins, fiona, shapely, pyproj, geopandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed click-8.1.7 click-plugins-1.1.1 cligj-0.7.2 contourpy-1.3.0 cycler-0.12.1 fiona-1.9.6 fonttools-4.55.0 geopandas-0.14.4 importlib-resources-6.4.5 jsonpickle-4.0.0 kiwisolver-1.4.7 matplotlib-3.7.2 networkx-3.1 numpy-1.24.4 pandas-2.0.3 pyparsing-3.0.9 pyproj-3.6.1 pyvis-0.3.2 shapely-2.0.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d7c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71915fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state   latitude   longitude\n",
      "0                Alabama  32.789907  -86.827783\n",
      "1                 Alaska  64.220419 -152.542689\n",
      "2                Arizona  34.293393 -111.663296\n",
      "3               Arkansas  34.898249  -92.440920\n",
      "4             California  37.253895 -119.614389\n",
      "5               Colorado  38.999340 -105.548773\n",
      "6            Connecticut  41.620895  -72.727856\n",
      "7               Delaware  38.982685  -75.497561\n",
      "8   District of Columbia  38.898378  -77.021809\n",
      "9                Florida  28.658894  -82.503970\n",
      "10               Georgia  32.647999  -83.446473\n",
      "11                Hawaii  20.148899 -156.217626\n",
      "12                 Idaho  44.388303 -114.659817\n",
      "13              Illinois  40.062673  -89.195740\n",
      "14               Indiana  39.912123  -86.270671\n",
      "15                  Iowa  42.075502  -93.500396\n",
      "16                Kansas  38.484404  -98.382772\n",
      "17              Kentucky  37.526932  -85.292111\n",
      "18             Louisiana  31.061171  -91.988564\n",
      "19                 Maine  45.378568  -69.232448\n",
      "20              Maryland  39.037021  -76.770575\n",
      "21         Massachusetts  42.267041  -71.843407\n",
      "22              Michigan  44.340358  -85.425582\n",
      "23             Minnesota  46.317018  -94.311237\n",
      "24           Mississippi  32.749802  -89.663501\n",
      "25              Missouri  38.368054  -92.477442\n",
      "26               Montana  47.033441 -109.646856\n",
      "27              Nebraska  41.527807  -99.810515\n",
      "28                Nevada  39.354828 -116.654356\n",
      "29         New Hampshire  43.681762  -71.579062\n",
      "30            New Jersey  40.192839  -74.661869\n",
      "31            New Mexico  34.421558 -106.107840\n",
      "32              New York  42.943357  -75.501110\n",
      "33        North Carolina  35.537693  -79.339049\n",
      "34          North Dakota  47.445465 -100.470121\n",
      "35                  Ohio  40.296073  -82.788206\n",
      "36              Oklahoma  35.584950  -97.505820\n",
      "37                Oregon  43.937318 -120.555327\n",
      "38          Pennsylvania  40.874137  -77.800038\n",
      "39          Rhode Island  41.691490  -71.570860\n",
      "40        South Carolina  33.903681  -80.895406\n",
      "41          South Dakota  44.435973 -100.232956\n",
      "42             Tennessee  35.842524  -86.347989\n",
      "43                 Texas  31.495173  -99.357873\n",
      "44                  Utah  39.325213 -111.677838\n",
      "45               Vermont  44.074399  -72.659474\n",
      "46              Virginia  37.515060  -78.789315\n",
      "47            Washington  47.375379 -120.448981\n",
      "48         West Virginia  38.638440  -80.616680\n",
      "49             Wisconsin  44.635045  -90.014225\n",
      "50               Wyoming  43.000697 -107.551902\n",
      "51           Puerto Rico  18.229197  -66.476307\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "file_path = \"./data/us-states.json\"\n",
    "    \n",
    "    # Read the GeoJSON data using geopandas\n",
    "with open(file_path, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "features = geojson_data[\"features\"]\n",
    "    \n",
    "# Create a list of geometries (Polygons)\n",
    "geometries = [shape(feature[\"geometry\"]) for feature in features]\n",
    "\n",
    "# Create a list of state names\n",
    "state_names = [feature[\"properties\"][\"name\"] for feature in features]\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame({'state': state_names, 'geometry': geometries})\n",
    "\n",
    "# Calculate centroids (latitude and longitude)\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "gdf['latitude'] = gdf['centroid'].apply(lambda x: x.y)\n",
    "gdf['longitude'] = gdf['centroid'].apply(lambda x: x.x)\n",
    "\n",
    "# Extract relevant columns\n",
    "state_coords = gdf[['state', 'latitude', 'longitude']]\n",
    "state_coords.to_csv(\"./data/state_coords.csv\", index=False) \n",
    "print(state_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7e1cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        respondent type type_name              period     value  revision_id  \\\n",
      "0             BANC    D    Demand 2022-01-02 02:00:00   2091.00       302352   \n",
      "1             PSEI    D    Demand 2022-01-02 02:00:00   4437.00       302352   \n",
      "2               SW    D    Demand 2022-01-02 02:00:00  12142.00       302352   \n",
      "3             WACM    D    Demand 2022-01-02 02:00:00   3212.00       302352   \n",
      "4             MISO    D    Demand 2022-01-02 02:00:00  74428.00       302352   \n",
      "...            ...  ...       ...                 ...       ...          ...   \n",
      "7715101        TEN    D    Demand 2024-07-04 06:00:00  20502.36       579043   \n",
      "7715102        GVL    D    Demand 2024-07-04 06:00:00    256.00       579043   \n",
      "7715103       PACW    D    Demand 2024-07-04 06:00:00   2495.00       579043   \n",
      "7715104       NEVP    D    Demand 2024-07-04 06:00:00   6843.00       579043   \n",
      "7715105       AZPS    D    Demand 2024-07-04 06:00:00   6251.00       579043   \n",
      "\n",
      "               id       state  \n",
      "0            7531  California  \n",
      "1            7532  California  \n",
      "2            7533     Arizona  \n",
      "3            7534     Arizona  \n",
      "4            7535    Michigan  \n",
      "...           ...         ...  \n",
      "7715101  17019460   Tennessee  \n",
      "7715102  17019461     Georgia  \n",
      "7715103  17019462  California  \n",
      "7715104  17019463      Nevada  \n",
      "7715105  17019464     Arizona  \n",
      "\n",
      "[2943879 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand mapped respondent to state\n",
    "respondent_to_state = {\n",
    "    'BANC': 'California', 'PSEI': 'California', 'SW': 'Arizona', 'WACM': 'Arizona', 'MISO': 'Michigan', 'SCEG': 'South Carolina',\n",
    "    'SPA': 'Texas', 'NY': 'New York', 'GVL': 'Georgia', 'FPL': 'Florida', 'PSCO': 'Colorado', 'DUK': 'North Carolina', \n",
    "    'ISNE': 'Massachusetts', 'HST': 'Texas', 'DOPD': 'Texas', 'US48': 'North America', 'PJM': 'Pennsylvania', 'AZPS': 'Arizona', \n",
    "    'CHPD': 'Texas', 'LDWP': 'California', 'SC': 'South Carolina', 'PNM': 'New Mexico', 'FMPP': 'Florida', 'FLA': 'Florida', \n",
    "    'SCL': 'California', 'IID': 'California', 'SWPP': 'Arkansas', 'WAUW': 'Washington', 'TEX': 'Texas', 'MIDA': 'Michigan', \n",
    "    'SOCO': 'Georgia', 'NEVP': 'Nevada', 'BPAT': 'Washington', 'ERCO': 'Texas', 'NW': 'Montana', 'CAR': 'North Carolina', \n",
    "    'FPC': 'Florida', 'GCPD': 'Texas', 'AECI': 'Missouri', 'PACW': 'California', 'MIDW': 'Wisconsin', 'CPLE': 'Florida', \n",
    "    'JEA': 'Florida', 'SRP': 'Arizona', 'PGE': 'California', 'TEN': 'Tennessee', 'CAL': 'California', 'IPCO': 'Oklahoma', \n",
    "    'AVA': 'Georgia', 'SEC': 'Texas', 'CISO': 'California', 'LGEE': 'Florida', 'TAL': 'Florida', 'TEC': 'Texas', \n",
    "    'NYIS': 'New York', 'TVA': 'Tennessee', 'CPLW': 'Texas', 'TPWR': 'Texas', 'CENT': 'Texas', 'TIDC': 'Texas', \n",
    "    'SE': 'Texas', 'WALC': 'Arizona', 'PACE': 'Utah', 'EPE': 'Texas', 'TEPC': 'Texas', 'NWMT': 'Montana', \n",
    "    'NE': 'Nebraska'\n",
    "}\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"./data/EIA930LoadAndForecast.csv\")\n",
    "\n",
    "data[\"state\"] = data[\"respondent\"].map(respondent_to_state)\n",
    "\n",
    "# Save the updated dataset\n",
    "data.to_csv(\"./data/EIA930LoadAndForecast_with_states.csv\", index=False) \n",
    "\n",
    "# Data cleaning and transformation\n",
    "data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "data['period'] = pd.to_datetime(data['period'])\n",
    "data = data.dropna().query(\"period.dt.year >= 2022\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837607f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87fd568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 5\n",
      "2 of 5\n",
      "3 of 5\n",
      "4 of 5\n",
      "5 of 5\n",
      "1471658\n",
      "Duplicates found in actuals before pivot:\n",
      "        respondent type type_name              period    value  revision_id  \\\n",
      "88              NE    D    Demand 2022-01-01 00:00:00  14859.0       302352   \n",
      "89             PNM    D    Demand 2022-01-01 00:00:00   1754.0       302352   \n",
      "90            LDWP    D    Demand 2022-01-01 00:00:00   2662.0       302352   \n",
      "91            BPAT    D    Demand 2022-01-01 00:00:00   8535.0       302352   \n",
      "92            CISO    D    Demand 2022-01-01 00:00:00  22618.0       302352   \n",
      "...            ...  ...       ...                 ...      ...          ...   \n",
      "2942730       BANC    D    Demand 2024-07-05 06:00:00   3115.0       579043   \n",
      "2942731       CPLW    D    Demand 2024-07-05 06:00:00    504.0       579043   \n",
      "2942732       TIDC    D    Demand 2024-07-05 06:00:00    520.0       579043   \n",
      "2942733        DUK    D    Demand 2024-07-05 06:00:00  12678.0       579043   \n",
      "2942734        TAL    D    Demand 2024-07-05 06:00:00    346.0       579043   \n",
      "\n",
      "               id           state  is_zero  is_negative  is_spike  impute  \\\n",
      "88              1        Nebraska        0            0         0       0   \n",
      "89              2      New Mexico        0            0         0       0   \n",
      "90              3      California        0            0         0       0   \n",
      "91              4      Washington        0            0         0       0   \n",
      "92              5      California        0            0         0       0   \n",
      "...           ...             ...      ...          ...       ...     ...   \n",
      "2942730  17026256      California        0            0         0       0   \n",
      "2942731  17026257           Texas        0            0         0       0   \n",
      "2942732  17026258           Texas        0            0         0       0   \n",
      "2942733  17026259  North Carolina        0            0         0       0   \n",
      "2942734  17026260         Florida        0            0         0       0   \n",
      "\n",
      "         imputed  is_imputed  \n",
      "88       14859.0           0  \n",
      "89        1754.0           0  \n",
      "90        2662.0           0  \n",
      "91        8535.0           0  \n",
      "92       22618.0           0  \n",
      "...          ...         ...  \n",
      "2942730   3115.0           0  \n",
      "2942731    504.0           0  \n",
      "2942732    520.0           0  \n",
      "2942733  12678.0           0  \n",
      "2942734    346.0           0  \n",
      "\n",
      "[6203 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mark anomalies\n",
    "def mark_anomalies(data):\n",
    "    data['is_zero'] = (data['value'] == 0).astype(int)\n",
    "    data['is_negative'] = (data['value'] < 0).astype(int)\n",
    "    \n",
    "    data['is_spike'] = data.groupby(['respondent', 'type_name'])['value'].transform(lambda x: (x > x.quantile(0.999)).astype(int))\n",
    "    data['is_spike'] = data['is_spike'].fillna(0)\n",
    "    return data\n",
    "\n",
    "# Impute data\n",
    "def impute_data(data):\n",
    "    # Mark impute column\n",
    "    data['impute'] = (data['is_zero'] + data['is_negative'] + data['is_spike'] > 0).astype(int)\n",
    "    \n",
    "    # Split into actuals and forecast\n",
    "    actuals = data[data['type_name'] == \"Demand\"]\n",
    "    forecast = data[data['type_name'] == \"Day-ahead demand forecast\"]\n",
    "    \n",
    "    # Merge actuals with forecast\n",
    "    joined = pd.merge(\n",
    "        actuals,\n",
    "        forecast.rename(columns={'value': 'forecast'}),\n",
    "        on=['respondent', 'period'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename columns to avoid suffixes like `_x` and `_y`\n",
    "    joined = joined.rename(columns={\n",
    "        'impute_x': 'impute',\n",
    "        'type_x': 'type',\n",
    "        'type_name_x': 'type_name',\n",
    "    })\n",
    "    \n",
    "    # Add imputed values\n",
    "    joined['imputed'] = np.where(\n",
    "        (joined['impute'] == 1) & ~joined['forecast'].isna(),\n",
    "        joined['forecast'],\n",
    "        np.where(\n",
    "            (joined['impute'] == 1) & joined['forecast'].isna() & ~joined['forecast'].shift(1).isna(),\n",
    "            joined['forecast'].shift(1),\n",
    "            np.where(\n",
    "                (joined['impute'] == 1) & joined['forecast'].isna() & joined['forecast'].shift(1).isna(),\n",
    "                joined['value'].shift(1),\n",
    "                joined['value']\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return cleaned data\n",
    "    return joined[['respondent', 'period', 'type', 'type_name', 'imputed']].rename(\n",
    "        columns={'imputed': 'value'}\n",
    "    ).drop_duplicates()\n",
    "data_marked = mark_anomalies(data)\n",
    "data_imputed = impute_data(data_marked)\n",
    "data_imputed.to_csv(\"./data/data_imputed.csv\", index=False)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"{i+1} of 5\")\n",
    "    data_marked = mark_anomalies(data_marked)\n",
    "    data_imputed = impute_data(data_marked)\n",
    "\n",
    "raw_imputed = pd.merge(data, data_imputed.rename(columns={'value': 'imputed'}),\n",
    "                       on=['respondent', 'type', 'type_name', 'period'], how='left')\n",
    "raw_imputed['is_imputed'] = (raw_imputed['value'] != raw_imputed['imputed']).astype(int)\n",
    "raw_imputed.to_csv(\"./data/raw_imputed.csv\", index=False)\n",
    "\n",
    "print(raw_imputed['is_imputed'].sum())\n",
    "# Calculate MAPEs\n",
    "actuals = raw_imputed[raw_imputed['type_name'] == \"Demand\"]\n",
    "forecast = raw_imputed[raw_imputed['type_name'] == \"Day-ahead demand forecast\"]\n",
    "joined = pd.merge(actuals, forecast[['respondent', 'period', 'value']].rename(columns={'value': 'forecast'}),\n",
    "                  on=['respondent', 'period'], how='left')\n",
    "joined['abs_error'] = np.abs(joined['value'] - joined['forecast']) / np.abs(joined['value'])\n",
    "\n",
    "MAPE = joined[joined['abs_error'] != np.inf].groupby('respondent')['abs_error'].mean().reset_index(name='MAPE')\n",
    "MAPE.to_csv(\"./data/MAPE.csv\", index=False)\n",
    "\n",
    "# Load edges and calculate correlations\n",
    "edges = pd.read_csv(\"./data/eia_930_edges.csv\")\n",
    "exclude = [\"CISO\", \"ERCO\", \"SWPP\", \"MISO\", \"NYIS\", \"ISNE\", \"CAL\", \"PJM\"]\n",
    "\n",
    "edges = edges.merge(MAPE, left_on=\"node1\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node1\"})\n",
    "edges = edges.merge(MAPE, left_on=\"node2\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node2\"})\n",
    "edges['abs_diff'] = np.abs(edges['MAPE_node1'] - edges['MAPE_node2'])\n",
    "edges = edges.query(\"~node1.isin(@exclude) & ~node2.isin(@exclude)\").sort_values('abs_diff', ascending=False)\n",
    "edges.to_csv(\"./data/edges_with_MAPE.csv\", index=False)\n",
    "\n",
    "# Wide format and correlation matrix\n",
    "duplicates = actuals[actuals.duplicated(subset=['period', 'respondent'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates found in actuals before pivot:\")\n",
    "    print(duplicates)\n",
    "    # dedup\n",
    "    actuals = actuals.drop_duplicates(subset=['period', 'respondent'])\n",
    "\n",
    "# Perform pivot operation\n",
    "actuals_wide = actuals.pivot(index='period', columns='respondent', values='imputed')\n",
    "correlation_matrix = actuals_wide.corr(method='pearson', min_periods=1)\n",
    "correlation_matrix.to_csv(\"./data/correlation_matrix.csv\")\n",
    "\n",
    "# Simple LDWP Model\n",
    "relevant_cols = ['CISO', 'BPAT', 'LDWP', 'PACE', 'NEVP', 'AZPS', 'WALC']\n",
    "reg_data = actuals_wide[relevant_cols].dropna()\n",
    "reg_data['LDWP_lag1'] = reg_data['LDWP'].shift(1)\n",
    "reg_data['LDWP_lag24'] = reg_data['LDWP'].shift(24)\n",
    "\n",
    "reg_data = reg_data.dropna()\n",
    "X = reg_data[['LDWP_lag1', 'LDWP_lag24', 'CISO', 'BPAT', 'PACE', 'NEVP', 'AZPS', 'WALC']]\n",
    "y = reg_data['LDWP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54ba0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linear Regression Model saved successfully.\n",
      "Best Random Forest Model saved successfully.\n",
      "Best Gradient Boosting Model saved successfully.\n",
      "Model Performance (MAPE):\n",
      "Linear Regression: 0.1275, Cross-Validation Mean: -103.7426\n",
      "Random Forest Regressor: 0.4140, Cross-Validation Mean: -98.6381\n",
      "Gradient Boosting Regressor: 0.6436, Cross-Validation Mean: -98.4495\n",
      "Evaluation results saved successfully.\n",
      "Linear Model Coefficients: [ 0.79309392  0.16407204 -0.00480436  0.0096736   0.06658138 -0.02581321\n",
      "  0.0076787   0.08533339]\n",
      "Random Forest Feature Importances: [0.95277565 0.01765398 0.01075002 0.00460675 0.00767984 0.00185537\n",
      " 0.00171825 0.00296015]\n",
      "Gradient Boosting Feature Importances: [9.50294019e-01 2.55986956e-02 9.58802399e-03 4.14949238e-03\n",
      " 6.83537775e-03 7.41416470e-04 8.79949126e-04 1.91302604e-03]\n",
      "Results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for GridSearchCV\n",
    "linear_param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, shuffle=True, random_state=614\n",
    "        )\n",
    "\n",
    "# Model 1: Linear Regression with GridSearchCV and Cross-Validation\n",
    "linear_model = LinearRegression()\n",
    "linear_grid_search = GridSearchCV(linear_model, linear_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Linear Regression\n",
    "linear_cv_results = cross_validate(linear_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_mean_score = linear_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Linear Regression Model\n",
    "best_linear_model = linear_grid_search.best_estimator_\n",
    "linear_predictions = best_linear_model.predict(X_test)\n",
    "linear_mape = mean_absolute_percentage_error(y_test, linear_predictions)\n",
    "with open(\"./models/linear_regression_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_linear_model, f)\n",
    "print(\"Best Linear Regression Model saved successfully.\")\n",
    "\n",
    "# Model 2: Random Forest Regressor with GridSearchCV and Cross-Validation\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "rf_cv_results = cross_validate(rf_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_mean_score = rf_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Random Forest Model\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "rf_mape = mean_absolute_percentage_error(y_test, rf_predictions)\n",
    "with open(\"./models/random_forest_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_rf_model, f)\n",
    "print(\"Best Random Forest Model saved successfully.\")\n",
    "\n",
    "# Model 3: Gradient Boosting Regressor with GridSearchCV and Cross-Validation\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid_search = GridSearchCV(gb_model, gb_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Gradient Boosting\n",
    "gb_cv_results = cross_validate(gb_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_mean_score = gb_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Gradient Boosting Model\n",
    "best_gb_model = gb_grid_search.best_estimator_\n",
    "gb_predictions = best_gb_model.predict(X_test)\n",
    "gb_mape = mean_absolute_percentage_error(y_test, gb_predictions)\n",
    "with open(\"./models/gradient_boosting_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_gb_model, f)\n",
    "print(\"Best Gradient Boosting Model saved successfully.\")\n",
    "\n",
    "# Compare model accuracy\n",
    "print(\"Model Performance (MAPE):\")\n",
    "print(f\"Linear Regression: {linear_mape:.4f}, Cross-Validation Mean: {linear_mean_score:.4f}\")\n",
    "print(f\"Random Forest Regressor: {rf_mape:.4f}, Cross-Validation Mean: {rf_mean_score:.4f}\")\n",
    "print(f\"Gradient Boosting Regressor: {gb_mape:.4f}, Cross-Validation Mean: {gb_mean_score:.4f}\")\n",
    "\n",
    "# Evaluation results dictionary\n",
    "evaluation_results = {\n",
    "    \"Linear Regression\": {\"MAPE\": linear_mape, \"Cross-Validation Mean\": linear_mean_score},\n",
    "    \"Random Forest\": {\"MAPE\": rf_mape, \"Cross-Validation Mean\": rf_mean_score},\n",
    "    \"Gradient Boosting\": {\"MAPE\": gb_mape, \"Cross-Validation Mean\": gb_mean_score},\n",
    "}\n",
    "\n",
    "# Save evaluation results to JSON\n",
    "with open(\"./data/evaluation_results.json\", \"w\") as file:\n",
    "    json.dump(evaluation_results, file)\n",
    "\n",
    "print(\"Evaluation results saved successfully.\")\n",
    "\n",
    "# Print model coefficients or feature importances\n",
    "print(\"Linear Model Coefficients:\", best_linear_model.coef_)\n",
    "print(\"Random Forest Feature Importances:\", best_rf_model.feature_importances_)\n",
    "print(\"Gradient Boosting Feature Importances:\", best_gb_model.feature_importances_)\n",
    "\n",
    "print(\"Results saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
