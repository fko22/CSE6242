{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc6ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy==1.24.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: pyvis==0.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: networkx==3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: matplotlib==3.7.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: geopandas==0.14.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.14.4)\n",
      "Requirement already satisfied: fiona==1.9.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.9.6)\n",
      "Collecting streamlit_folium==0.23.2\n",
      "  Downloading streamlit_folium-0.23.2-py3-none-any.whl (328 kB)\n",
      "\u001b[K     |████████████████████████████████| 328 kB 650 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (8.12.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (4.55.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (6.4.5)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from geopandas==0.14.4->-r requirements.txt (line 7)) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from geopandas==0.14.4->-r requirements.txt (line 7)) (2.0.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (24.2.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (2024.8.30)\n",
      "Requirement already satisfied: cligj>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (8.5.0)\n",
      "Requirement already satisfied: six in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: click~=8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (1.1.1)\n",
      "Collecting folium!=0.15.0,>=0.13\n",
      "  Downloading folium-0.18.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting branca\n",
      "  Downloading branca-0.8.0-py3-none-any.whl (25 kB)\n",
      "Collecting streamlit>=1.13.0\n",
      "  Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 17.0 MB/s eta 0:00:01     |█████                           | 1.3 MB 17.0 MB/s eta 0:00:01     |███████▋                        | 2.0 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2>=2.9.6->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.14.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.48)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.5)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: decorator in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib==3.7.2->-r requirements.txt (line 6)) (3.20.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2.32.3)\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tenacity<10,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (13.9.3)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.2.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.8.0b4)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.25.3)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (6.4.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.0.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.23.0)\n",
      "Requirement already satisfied: toolz in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: entrypoints in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.1.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.35.1)\n",
      "Installing collected packages: branca, xyzservices, folium, smmap, gitdb, gitpython, cachetools, blinker, toml, streamlit, streamlit-folium\n",
      "Successfully installed blinker-1.9.0 branca-0.8.0 cachetools-5.5.0 folium-0.18.0 gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1 streamlit-1.40.1 streamlit-folium-0.23.2 toml-0.10.2 xyzservices-2024.9.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d7c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from app import extract_zip\n",
    "\n",
    "def compress_to_zip(filename, df):\n",
    "    csv_path = f\"./data/{filename}.csv\"\n",
    "    zip_path = f\"./data/{filename}.zip\"\n",
    "    df.to_csv(csv_path, index=False) \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "    print(f\"DataFrame saved and compressed into: {zip_path}\")\n",
    "    os.remove(csv_path)\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "file_path = \"./data/us-states.json\"\n",
    "    \n",
    "    # Read the GeoJSON data using geopandas\n",
    "with open(file_path, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "features = geojson_data[\"features\"]\n",
    "    \n",
    "# Create a list of geometries (Polygons)\n",
    "geometries = [shape(feature[\"geometry\"]) for feature in features]\n",
    "\n",
    "# Create a list of state names\n",
    "state_names = [feature[\"properties\"][\"name\"] for feature in features]\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame({'state': state_names, 'geometry': geometries})\n",
    "\n",
    "# Calculate centroids (latitude and longitude)\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "gdf['latitude'] = gdf['centroid'].apply(lambda x: x.y)\n",
    "gdf['longitude'] = gdf['centroid'].apply(lambda x: x.x)\n",
    "\n",
    "# Extract relevant columns\n",
    "\n",
    "state_coordinates = gdf[['state', 'latitude', 'longitude', 'geometry']]\n",
    "print(state_coordinates.head())\n",
    "\n",
    "compress_to_zip(\"state_coordinates\",state_coordinates)\n",
    "\n",
    "# Hand mapped respondent to state\n",
    "respondent_to_state = {\n",
    "    'BANC': 'California', 'PSEI': 'California', 'SW': 'Arizona', 'WACM': 'Arizona', 'MISO': 'Michigan', 'SCEG': 'South Carolina',\n",
    "    'SPA': 'Texas', 'NY': 'New York', 'GVL': 'Georgia', 'FPL': 'Florida', 'PSCO': 'Colorado', 'DUK': 'North Carolina', \n",
    "    'ISNE': 'Massachusetts', 'HST': 'Texas', 'DOPD': 'Texas', 'US48': 'North America', 'PJM': 'Pennsylvania', 'AZPS': 'Arizona', \n",
    "    'CHPD': 'Texas', 'LDWP': 'California', 'SC': 'South Carolina', 'PNM': 'New Mexico', 'FMPP': 'Florida', 'FLA': 'Florida', \n",
    "    'SCL': 'California', 'IID': 'California', 'SWPP': 'Arkansas', 'WAUW': 'Washington', 'TEX': 'Texas', 'MIDA': 'Michigan', \n",
    "    'SOCO': 'Georgia', 'NEVP': 'Nevada', 'BPAT': 'Washington', 'ERCO': 'Texas', 'NW': 'Montana', 'CAR': 'North Carolina', \n",
    "    'FPC': 'Florida', 'GCPD': 'Texas', 'AECI': 'Missouri', 'PACW': 'California', 'MIDW': 'Wisconsin', 'CPLE': 'Florida', \n",
    "    'JEA': 'Florida', 'SRP': 'Arizona', 'PGE': 'California', 'TEN': 'Tennessee', 'CAL': 'California', 'IPCO': 'Oklahoma', \n",
    "    'AVA': 'Georgia', 'SEC': 'Texas', 'CISO': 'California', 'LGEE': 'Florida', 'TAL': 'Florida', 'TEC': 'Texas', \n",
    "    'NYIS': 'New York', 'TVA': 'Tennessee', 'CPLW': 'Texas', 'TPWR': 'Texas', 'CENT': 'Texas', 'TIDC': 'Texas', \n",
    "    'SE': 'Texas', 'WALC': 'Arizona', 'PACE': 'Utah', 'EPE': 'Texas', 'TEPC': 'Texas', 'NWMT': 'Montana', \n",
    "    'NE': 'Nebraska'\n",
    "}\n",
    "\n",
    "# Load data\n",
    "data = extract_zip(\"EIA930LoadAndForecast\")\n",
    "\n",
    "data[\"state\"] = data[\"respondent\"].map(respondent_to_state)\n",
    "\n",
    "# Save the updated dataset\n",
    "compress_to_zip(\"EIA930LoadAndForecast_with_states\",data)\n",
    "# Data cleaning and transformation\n",
    "data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "data['period'] = pd.to_datetime(data['period'])\n",
    "data = data.dropna().query(\"period.dt.year >= 2022\")\n",
    "print(data)\n",
    "\n",
    "\n",
    "# Mark anomalies\n",
    "def mark_anomalies(data):\n",
    "    data['is_zero'] = (data['value'] == 0).astype(int)\n",
    "    data['is_negative'] = (data['value'] < 0).astype(int)\n",
    "    \n",
    "    data['is_spike'] = data.groupby(['respondent', 'type_name'])['value'].transform(lambda x: (x > x.quantile(0.999)).astype(int))\n",
    "    data['is_spike'] = data['is_spike'].fillna(0)\n",
    "    return data\n",
    "\n",
    "# Impute data\n",
    "def impute_data(data):\n",
    "    # Mark impute column\n",
    "    data['impute'] = (data['is_zero'] + data['is_negative'] + data['is_spike'] > 0).astype(int)\n",
    "    \n",
    "    # Split into actuals and forecast\n",
    "    actuals = data[data['type_name'] == \"Demand\"]\n",
    "    forecast = data[data['type_name'] == \"Day-ahead demand forecast\"]\n",
    "    \n",
    "    # Merge actuals with forecast\n",
    "    joined = pd.merge(\n",
    "        actuals,\n",
    "        forecast.rename(columns={'value': 'forecast'}),\n",
    "        on=['respondent', 'period'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename columns to avoid suffixes like `_x` and `_y`\n",
    "    joined = joined.rename(columns={\n",
    "        'impute_x': 'impute',\n",
    "        'type_x': 'type',\n",
    "        'type_name_x': 'type_name',\n",
    "    })\n",
    "    \n",
    "    # Add imputed values\n",
    "    joined['imputed'] = np.where(\n",
    "        (joined['impute'] == 1) & ~joined['forecast'].isna(),\n",
    "        joined['forecast'],\n",
    "        np.where(\n",
    "            (joined['impute'] == 1) & joined['forecast'].isna() & ~joined['forecast'].shift(1).isna(),\n",
    "            joined['forecast'].shift(1),\n",
    "            np.where(\n",
    "                (joined['impute'] == 1) & joined['forecast'].isna() & joined['forecast'].shift(1).isna(),\n",
    "                joined['value'].shift(1),\n",
    "                joined['value']\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return cleaned data\n",
    "    return joined[['respondent', 'period', 'type', 'type_name', 'imputed']].rename(\n",
    "        columns={'imputed': 'value'}\n",
    "    ).drop_duplicates()\n",
    "data_marked = mark_anomalies(data)\n",
    "data_imputed = impute_data(data_marked)\n",
    "\n",
    "# Save the updated dataset\n",
    "compress_to_zip(\"data_imputed\",data_imputed)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"{i+1} of 5\")\n",
    "    data_marked = mark_anomalies(data_marked)\n",
    "    data_imputed = impute_data(data_marked)\n",
    "\n",
    "raw_imputed = pd.merge(data, data_imputed.rename(columns={'value': 'imputed'}),\n",
    "                       on=['respondent', 'type', 'type_name', 'period'], how='left')\n",
    "raw_imputed['is_imputed'] = (raw_imputed['value'] != raw_imputed['imputed']).astype(int)\n",
    "\n",
    "compress_to_zip(\"raw_imputed\",raw_imputed)\n",
    "\n",
    "print(raw_imputed['is_imputed'].sum())\n",
    "# Calculate MAPEs\n",
    "actuals = raw_imputed[raw_imputed['type_name'] == \"Demand\"]\n",
    "forecast = raw_imputed[raw_imputed['type_name'] == \"Day-ahead demand forecast\"]\n",
    "joined = pd.merge(actuals, forecast[['respondent', 'period', 'value']].rename(columns={'value': 'forecast'}),\n",
    "                  on=['respondent', 'period'], how='left')\n",
    "joined['abs_error'] = np.abs(joined['value'] - joined['forecast']) / np.abs(joined['value'])\n",
    "\n",
    "MAPE = joined[joined['abs_error'] != np.inf].groupby('respondent')['abs_error'].mean().reset_index(name='MAPE')\n",
    "compress_to_zip(\"MAPE\",MAPE)\n",
    "\n",
    "# Load edges and calculate correlations\n",
    "edges = extract_zip(\"eia_930_edges\")\n",
    "exclude = [\"CISO\", \"ERCO\", \"SWPP\", \"MISO\", \"NYIS\", \"ISNE\", \"CAL\", \"PJM\"]\n",
    "\n",
    "edges = edges.merge(MAPE, left_on=\"node1\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node1\"})\n",
    "edges = edges.merge(MAPE, left_on=\"node2\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node2\"})\n",
    "edges['abs_diff'] = np.abs(edges['MAPE_node1'] - edges['MAPE_node2'])\n",
    "edges = edges.query(\"~node1.isin(@exclude) & ~node2.isin(@exclude)\").sort_values('abs_diff', ascending=False)\n",
    "compress_to_zip(\"edges_with_MAPE\",edges)\n",
    "\n",
    "# Wide format and correlation matrix\n",
    "duplicates = actuals[actuals.duplicated(subset=['period', 'respondent'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates found in actuals before pivot:\")\n",
    "    print(duplicates)\n",
    "    # dedup\n",
    "    actuals = actuals.drop_duplicates(subset=['period', 'respondent'])\n",
    "\n",
    "# Perform pivot operation\n",
    "actuals_wide = actuals.pivot(index='period', columns='respondent', values='imputed')\n",
    "correlation_matrix = actuals_wide.corr(method='pearson', min_periods=1)\n",
    "compress_to_zip(\"correlation_matrix\",correlation_matrix)\n",
    "\n",
    "# Simple LDWP Model\n",
    "relevant_cols = ['CISO', 'BPAT', 'LDWP', 'PACE', 'NEVP', 'AZPS', 'WALC']\n",
    "reg_data = actuals_wide[relevant_cols].dropna()\n",
    "reg_data['LDWP_lag1'] = reg_data['LDWP'].shift(1)\n",
    "reg_data['LDWP_lag24'] = reg_data['LDWP'].shift(24)\n",
    "\n",
    "reg_data = reg_data.dropna()\n",
    "X = reg_data[['LDWP_lag1', 'LDWP_lag24', 'CISO', 'BPAT', 'PACE', 'NEVP', 'AZPS', 'WALC']]\n",
    "y = reg_data['LDWP']\n",
    "\n",
    "\n",
    "# Define hyperparameter grids for GridSearchCV\n",
    "linear_param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, shuffle=True, random_state=614\n",
    "        )\n",
    "\n",
    "# Model 1: Linear Regression with GridSearchCV and Cross-Validation\n",
    "linear_model = LinearRegression()\n",
    "linear_grid_search = GridSearchCV(linear_model, linear_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Linear Regression\n",
    "linear_cv_results = cross_validate(linear_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_mean_score = linear_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Linear Regression Model\n",
    "best_linear_model = linear_grid_search.best_estimator_\n",
    "linear_predictions = best_linear_model.predict(X_test)\n",
    "linear_mape = mean_absolute_percentage_error(y_test, linear_predictions)\n",
    "with open(\"./models/linear_regression_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_linear_model, f)\n",
    "print(\"Best Linear Regression Model saved successfully.\")\n",
    "\n",
    "# Model 2: Random Forest Regressor with GridSearchCV and Cross-Validation\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "rf_cv_results = cross_validate(rf_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_mean_score = rf_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Random Forest Model\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "rf_mape = mean_absolute_percentage_error(y_test, rf_predictions)\n",
    "with open(\"./models/random_forest_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_rf_model, f)\n",
    "print(\"Best Random Forest Model saved successfully.\")\n",
    "\n",
    "# Model 3: Gradient Boosting Regressor with GridSearchCV and Cross-Validation\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid_search = GridSearchCV(gb_model, gb_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Gradient Boosting\n",
    "gb_cv_results = cross_validate(gb_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_mean_score = gb_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Gradient Boosting Model\n",
    "best_gb_model = gb_grid_search.best_estimator_\n",
    "gb_predictions = best_gb_model.predict(X_test)\n",
    "gb_mape = mean_absolute_percentage_error(y_test, gb_predictions)\n",
    "with open(\"./models/gradient_boosting_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_gb_model, f)\n",
    "print(\"Best Gradient Boosting Model saved successfully.\")\n",
    "\n",
    "# Compare model accuracy\n",
    "print(\"Model Performance (MAPE):\")\n",
    "print(f\"Linear Regression: {linear_mape:.4f}, Cross-Validation Mean: {linear_mean_score:.4f}\")\n",
    "print(f\"Random Forest Regressor: {rf_mape:.4f}, Cross-Validation Mean: {rf_mean_score:.4f}\")\n",
    "print(f\"Gradient Boosting Regressor: {gb_mape:.4f}, Cross-Validation Mean: {gb_mean_score:.4f}\")\n",
    "\n",
    "# Evaluation results dictionary\n",
    "evaluation_results = {\n",
    "    \"Linear Regression\": {\"MAPE\": linear_mape, \"Cross-Validation Mean\": linear_mean_score},\n",
    "    \"Random Forest\": {\"MAPE\": rf_mape, \"Cross-Validation Mean\": rf_mean_score},\n",
    "    \"Gradient Boosting\": {\"MAPE\": gb_mape, \"Cross-Validation Mean\": gb_mean_score},\n",
    "}\n",
    "\n",
    "# Save evaluation results to JSON\n",
    "with open(\"./data/evaluation_results.json\", \"w\") as file:\n",
    "    json.dump(evaluation_results, file)\n",
    "\n",
    "print(\"Evaluation results saved successfully.\")\n",
    "\n",
    "# Print model coefficients or feature importances\n",
    "print(\"Linear Model Coefficients:\", best_linear_model.coef_)\n",
    "print(\"Random Forest Feature Importances:\", best_rf_model.feature_importances_)\n",
    "print(\"Gradient Boosting Feature Importances:\", best_gb_model.feature_importances_)\n",
    "\n",
    "print(\"Results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8916fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to: ./data/edges_with_MAPE.csv\n",
      "    node1 node2 respondent_x  MAPE_node1 respondent_y  MAPE_node2  abs_diff\n",
      "0    AECI   SPA         AECI    0.036209          SPA    1.044349  1.008140\n",
      "1     SPA  AECI          SPA    1.044349         AECI    0.036209  1.008140\n",
      "2    PSCO   PNM         PSCO    0.723519          PNM    0.063648  0.659871\n",
      "3     PNM  PSCO          PNM    0.063648         PSCO    0.723519  0.659871\n",
      "4    PSCO  WACM         PSCO    0.723519         WACM    0.142075  0.581444\n",
      "..    ...   ...          ...         ...          ...         ...       ...\n",
      "187  GCPD  BPAT         GCPD    0.023107         BPAT    0.020857  0.002250\n",
      "188  BPAT   AVA         BPAT    0.020857          AVA    0.019132  0.001726\n",
      "189   AVA  BPAT          AVA    0.019132         BPAT    0.020857  0.001726\n",
      "190   TEC  FMPP          TEC    0.046723         FMPP    0.045600  0.001123\n",
      "191  FMPP   TEC         FMPP    0.045600          TEC    0.046723  0.001123\n",
      "\n",
      "[192 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "edges = extract_zip(\"edges_with_MAPE\")\n",
    "print(edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
