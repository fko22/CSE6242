{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc6ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy==1.24.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: pyvis==0.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: networkx==3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: matplotlib==3.7.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: geopandas==0.14.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.14.4)\n",
      "Requirement already satisfied: fiona==1.9.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.9.6)\n",
      "Collecting streamlit_folium==0.23.2\n",
      "  Downloading streamlit_folium-0.23.2-py3-none-any.whl (328 kB)\n",
      "\u001b[K     |████████████████████████████████| 328 kB 650 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pandas==2.0.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyvis==0.3.2->-r requirements.txt (line 4)) (8.12.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (4.55.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib==3.7.2->-r requirements.txt (line 6)) (6.4.5)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from geopandas==0.14.4->-r requirements.txt (line 7)) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from geopandas==0.14.4->-r requirements.txt (line 7)) (2.0.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (24.2.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (2024.8.30)\n",
      "Requirement already satisfied: cligj>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (8.5.0)\n",
      "Requirement already satisfied: six in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: click~=8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fiona==1.9.6->-r requirements.txt (line 8)) (1.1.1)\n",
      "Collecting folium!=0.15.0,>=0.13\n",
      "  Downloading folium-0.18.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting branca\n",
      "  Downloading branca-0.8.0-py3-none-any.whl (25 kB)\n",
      "Collecting streamlit>=1.13.0\n",
      "  Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 17.0 MB/s eta 0:00:01     |█████                           | 1.3 MB 17.0 MB/s eta 0:00:01     |███████▋                        | 2.0 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2>=2.9.6->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.14.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (3.0.48)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.5)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: decorator in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib==3.7.2->-r requirements.txt (line 6)) (3.20.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2.32.3)\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tenacity<10,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (13.9.3)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.2.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.8.0b4)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.25.3)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (6.4.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.2.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/franciso/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.3.0->pyvis==0.3.2->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->folium!=0.15.0,>=0.13->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (3.0.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (4.23.0)\n",
      "Requirement already satisfied: toolz in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: entrypoints in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.1.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.13.0->streamlit_folium==0.23.2->-r requirements.txt (line 9)) (0.35.1)\n",
      "Installing collected packages: branca, xyzservices, folium, smmap, gitdb, gitpython, cachetools, blinker, toml, streamlit, streamlit-folium\n",
      "Successfully installed blinker-1.9.0 branca-0.8.0 cachetools-5.5.0 folium-0.18.0 gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1 streamlit-1.40.1 streamlit-folium-0.23.2 toml-0.10.2 xyzservices-2024.9.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d7c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from app import extract_zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7ba36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved and compressed into: ./data/Balancing_Authority_Lat_Long.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compress_to_zip(filename, df):\n",
    "    csv_path = f\"./data/{filename}.csv\"\n",
    "    zip_path = f\"./data/{filename}.zip\"\n",
    "    df.to_csv(csv_path, index=False) \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "    print(f\"DataFrame saved and compressed into: {zip_path}\")\n",
    "    os.remove(csv_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a9988f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state   latitude   longitude  \\\n",
      "0     Alabama  32.789907  -86.827783   \n",
      "1      Alaska  64.220419 -152.542689   \n",
      "2     Arizona  34.293393 -111.663296   \n",
      "3    Arkansas  34.898249  -92.440920   \n",
      "4  California  37.253895 -119.614389   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-87.359 35.001, -85.607 34.985, -85....  \n",
      "1  MULTIPOLYGON (((-131.602 55.118, -131.569 55.2...  \n",
      "2  POLYGON ((-109.043 37.000, -109.048 31.332, -1...  \n",
      "3  POLYGON ((-94.474 36.502, -90.153 36.496, -90....  \n",
      "4  POLYGON ((-123.233 42.006, -122.379 42.012, -1...  \n",
      "DataFrame saved and compressed into: ./data/state_coordinates.zip\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "file_path = \"./data/us-states.json\"\n",
    "    \n",
    "    # Read the GeoJSON data using geopandas\n",
    "with open(file_path, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "features = geojson_data[\"features\"]\n",
    "    \n",
    "# Create a list of geometries (Polygons)\n",
    "geometries = [shape(feature[\"geometry\"]) for feature in features]\n",
    "\n",
    "# Create a list of state names\n",
    "state_names = [feature[\"properties\"][\"name\"] for feature in features]\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame({'state': state_names, 'geometry': geometries})\n",
    "\n",
    "# Calculate centroids (latitude and longitude)\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "gdf['latitude'] = gdf['centroid'].apply(lambda x: x.y)\n",
    "gdf['longitude'] = gdf['centroid'].apply(lambda x: x.x)\n",
    "\n",
    "# Extract relevant columns\n",
    "\n",
    "state_coordinates = gdf[['state', 'latitude', 'longitude', 'geometry']]\n",
    "print(state_coordinates.head())\n",
    "\n",
    "compress_to_zip(\"state_coordinates\",state_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ac1c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to: ./data/EIA930LoadAndForecast.csv\n",
      "DataFrame saved and compressed into: ./data/EIA930LoadAndForecast_with_states.zip\n",
      "        respondent type type_name              period     value  revision_id  \\\n",
      "0             BANC    D    Demand 2022-01-02 02:00:00   2091.00       302352   \n",
      "1             PSEI    D    Demand 2022-01-02 02:00:00   4437.00       302352   \n",
      "2               SW    D    Demand 2022-01-02 02:00:00  12142.00       302352   \n",
      "3             WACM    D    Demand 2022-01-02 02:00:00   3212.00       302352   \n",
      "4             MISO    D    Demand 2022-01-02 02:00:00  74428.00       302352   \n",
      "...            ...  ...       ...                 ...       ...          ...   \n",
      "7715101        TEN    D    Demand 2024-07-04 06:00:00  20502.36       579043   \n",
      "7715102        GVL    D    Demand 2024-07-04 06:00:00    256.00       579043   \n",
      "7715103       PACW    D    Demand 2024-07-04 06:00:00   2495.00       579043   \n",
      "7715104       NEVP    D    Demand 2024-07-04 06:00:00   6843.00       579043   \n",
      "7715105       AZPS    D    Demand 2024-07-04 06:00:00   6251.00       579043   \n",
      "\n",
      "               id       state  Monday  Tuesday  Wednesday  Thursday  Friday  \\\n",
      "0            7531  California       0        0          0         0       0   \n",
      "1            7532  California       0        0          0         0       0   \n",
      "2            7533     Arizona       0        0          0         0       0   \n",
      "3            7534     Arizona       0        0          0         0       0   \n",
      "4            7535    Michigan       0        0          0         0       0   \n",
      "...           ...         ...     ...      ...        ...       ...     ...   \n",
      "7715101  17019460   Tennessee       0        0          0         1       0   \n",
      "7715102  17019461     Georgia       0        0          0         1       0   \n",
      "7715103  17019462  California       0        0          0         1       0   \n",
      "7715104  17019463      Nevada       0        0          0         1       0   \n",
      "7715105  17019464     Arizona       0        0          0         1       0   \n",
      "\n",
      "         Sunday  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "...         ...  \n",
      "7715101       0  \n",
      "7715102       0  \n",
      "7715103       0  \n",
      "7715104       0  \n",
      "7715105       0  \n",
      "\n",
      "[2943879 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand mapped respondent to state\n",
    "respondent_to_state = {\n",
    "    'BANC': 'California', 'PSEI': 'California', 'SW': 'Arizona', 'WACM': 'Arizona', 'MISO': 'Michigan', 'SCEG': 'South Carolina',\n",
    "    'SPA': 'Texas', 'NY': 'New York', 'GVL': 'Georgia', 'FPL': 'Florida', 'PSCO': 'Colorado', 'DUK': 'North Carolina', \n",
    "    'ISNE': 'Massachusetts', 'HST': 'Texas', 'DOPD': 'Texas', 'US48': 'North America', 'PJM': 'Pennsylvania', 'AZPS': 'Arizona', \n",
    "    'CHPD': 'Texas', 'LDWP': 'California', 'SC': 'South Carolina', 'PNM': 'New Mexico', 'FMPP': 'Florida', 'FLA': 'Florida', \n",
    "    'SCL': 'California', 'IID': 'California', 'SWPP': 'Arkansas', 'WAUW': 'Washington', 'TEX': 'Texas', 'MIDA': 'Michigan', \n",
    "    'SOCO': 'Georgia', 'NEVP': 'Nevada', 'BPAT': 'Washington', 'ERCO': 'Texas', 'NW': 'Montana', 'CAR': 'North Carolina', \n",
    "    'FPC': 'Florida', 'GCPD': 'Texas', 'AECI': 'Missouri', 'PACW': 'California', 'MIDW': 'Wisconsin', 'CPLE': 'Florida', \n",
    "    'JEA': 'Florida', 'SRP': 'Arizona', 'PGE': 'California', 'TEN': 'Tennessee', 'CAL': 'California', 'IPCO': 'Oklahoma', \n",
    "    'AVA': 'Georgia', 'SEC': 'Texas', 'CISO': 'California', 'LGEE': 'Florida', 'TAL': 'Florida', 'TEC': 'Texas', \n",
    "    'NYIS': 'New York', 'TVA': 'Tennessee', 'CPLW': 'Texas', 'TPWR': 'Texas', 'CENT': 'Texas', 'TIDC': 'Texas', \n",
    "    'SE': 'Texas', 'WALC': 'Arizona', 'PACE': 'Utah', 'EPE': 'Texas', 'TEPC': 'Texas', 'NWMT': 'Montana', \n",
    "    'NE': 'Nebraska'\n",
    "}\n",
    "\n",
    "# Load data\n",
    "data = extract_zip(\"EIA930LoadAndForecast\")\n",
    "\n",
    "data[\"state\"] = data[\"respondent\"].map(respondent_to_state)\n",
    "\n",
    "# Save the updated dataset\n",
    "compress_to_zip(\"EIA930LoadAndForecast_with_states\",data)\n",
    "# Data cleaning and transformation\n",
    "data['value'] = pd.to_numeric(data['value'], errors='coerce')\n",
    "data['period'] = pd.to_datetime(data['period'])\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday']\n",
    "for day in days:\n",
    "    data[day] = (data['period'].dt.day_name() == day).astype(int)\n",
    "data = data.dropna().query(\"period.dt.year >= 2022\")\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aba4f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved and compressed into: ./data/data_imputed.zip\n",
      "1 of 5\n",
      "2 of 5\n",
      "3 of 5\n",
      "4 of 5\n",
      "5 of 5\n",
      "DataFrame saved and compressed into: ./data/raw_imputed.zip\n",
      "1471658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mark anomalies\n",
    "def mark_anomalies(data):\n",
    "    data['is_zero'] = (data['value'] == 0).astype(int)\n",
    "    data['is_negative'] = (data['value'] < 0).astype(int)\n",
    "    \n",
    "    data['is_spike'] = data.groupby(['respondent', 'type_name'])['value'].transform(lambda x: (x > x.quantile(0.999)).astype(int))\n",
    "    data['is_spike'] = data['is_spike'].fillna(0)\n",
    "    return data\n",
    "\n",
    "# Impute data\n",
    "def impute_data(data):\n",
    "    # Mark impute column\n",
    "    data['impute'] = (data['is_zero'] + data['is_negative'] + data['is_spike'] > 0).astype(int)\n",
    "    \n",
    "    # Split into actuals and forecast\n",
    "    actuals = data[data['type_name'] == \"Demand\"]\n",
    "    forecast = data[data['type_name'] == \"Day-ahead demand forecast\"]\n",
    "    \n",
    "    # Merge actuals with forecast\n",
    "    joined = pd.merge(\n",
    "        actuals,\n",
    "        forecast.rename(columns={'value': 'forecast'}),\n",
    "        on=['respondent', 'period'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename columns to avoid suffixes like `_x` and `_y`\n",
    "    joined = joined.rename(columns={\n",
    "        'impute_x': 'impute',\n",
    "        'type_x': 'type',\n",
    "        'type_name_x': 'type_name',\n",
    "    })\n",
    "    \n",
    "    # Add imputed values\n",
    "    joined['imputed'] = np.where(\n",
    "        (joined['impute'] == 1) & ~joined['forecast'].isna(),\n",
    "        joined['forecast'],\n",
    "        np.where(\n",
    "            (joined['impute'] == 1) & joined['forecast'].isna() & ~joined['forecast'].shift(1).isna(),\n",
    "            joined['forecast'].shift(1),\n",
    "            np.where(\n",
    "                (joined['impute'] == 1) & joined['forecast'].isna() & joined['forecast'].shift(1).isna(),\n",
    "                joined['value'].shift(1),\n",
    "                joined['value']\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return cleaned data\n",
    "    return joined[['respondent', 'period', 'type', 'type_name', 'imputed']].rename(\n",
    "        columns={'imputed': 'value'}\n",
    "    ).drop_duplicates()\n",
    "data_marked = mark_anomalies(data)\n",
    "data_imputed = impute_data(data_marked)\n",
    "\n",
    "# Save the updated dataset\n",
    "compress_to_zip(\"data_imputed\",data_imputed)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"{i+1} of 5\")\n",
    "    data_marked = mark_anomalies(data_marked)\n",
    "    data_imputed = impute_data(data_marked)\n",
    "\n",
    "raw_imputed = pd.merge(data, data_imputed.rename(columns={'value': 'imputed'}),\n",
    "                       on=['respondent', 'type', 'type_name', 'period'], how='left')\n",
    "raw_imputed['is_imputed'] = (raw_imputed['value'] != raw_imputed['imputed']).astype(int)\n",
    "\n",
    "compress_to_zip(\"raw_imputed\",raw_imputed)\n",
    "\n",
    "print(raw_imputed['is_imputed'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbd4fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved and compressed into: ./data/MAPE.zip\n",
      "Files extracted to: ./data/eia_930_edges.csv\n",
      "DataFrame saved and compressed into: ./data/edges_with_MAPE.zip\n",
      "    node1 node2 respondent_x  MAPE_node1 respondent_y  MAPE_node2  abs_diff\n",
      "7    AECI   SPA         AECI    0.036209          SPA    1.044349  1.008140\n",
      "239   SPA  AECI          SPA    1.044349         AECI    0.036209  1.008140\n",
      "95   PSCO   PNM         PSCO    0.723519          PNM    0.063648  0.659871\n",
      "252   PNM  PSCO          PNM    0.063648         PSCO    0.723519  0.659871\n",
      "113  PSCO  WACM         PSCO    0.723519         WACM    0.142075  0.581444\n",
      "..    ...   ...          ...         ...          ...         ...       ...\n",
      "31   GCPD  BPAT         GCPD    0.023107         BPAT    0.020857  0.002250\n",
      "127  BPAT   AVA         BPAT    0.020857          AVA    0.019132  0.001726\n",
      "26    AVA  BPAT          AVA    0.019132         BPAT    0.020857  0.001726\n",
      "226   TEC  FMPP          TEC    0.046723         FMPP    0.045600  0.001123\n",
      "219  FMPP   TEC         FMPP    0.045600          TEC    0.046723  0.001123\n",
      "\n",
      "[192 rows x 7 columns]\n",
      "Duplicates found in actuals before pivot:\n",
      "        respondent type type_name              period    value  revision_id  \\\n",
      "88              NE    D    Demand 2022-01-01 00:00:00  14859.0       302352   \n",
      "89             PNM    D    Demand 2022-01-01 00:00:00   1754.0       302352   \n",
      "90            LDWP    D    Demand 2022-01-01 00:00:00   2662.0       302352   \n",
      "91            BPAT    D    Demand 2022-01-01 00:00:00   8535.0       302352   \n",
      "92            CISO    D    Demand 2022-01-01 00:00:00  22618.0       302352   \n",
      "...            ...  ...       ...                 ...      ...          ...   \n",
      "2942730       BANC    D    Demand 2024-07-05 06:00:00   3115.0       579043   \n",
      "2942731       CPLW    D    Demand 2024-07-05 06:00:00    504.0       579043   \n",
      "2942732       TIDC    D    Demand 2024-07-05 06:00:00    520.0       579043   \n",
      "2942733        DUK    D    Demand 2024-07-05 06:00:00  12678.0       579043   \n",
      "2942734        TAL    D    Demand 2024-07-05 06:00:00    346.0       579043   \n",
      "\n",
      "               id           state  Monday  Tuesday  Wednesday  Thursday  \\\n",
      "88              1        Nebraska       0        0          0         0   \n",
      "89              2      New Mexico       0        0          0         0   \n",
      "90              3      California       0        0          0         0   \n",
      "91              4      Washington       0        0          0         0   \n",
      "92              5      California       0        0          0         0   \n",
      "...           ...             ...     ...      ...        ...       ...   \n",
      "2942730  17026256      California       0        0          0         0   \n",
      "2942731  17026257           Texas       0        0          0         0   \n",
      "2942732  17026258           Texas       0        0          0         0   \n",
      "2942733  17026259  North Carolina       0        0          0         0   \n",
      "2942734  17026260         Florida       0        0          0         0   \n",
      "\n",
      "         Friday  Sunday  is_zero  is_negative  is_spike  impute  imputed  \\\n",
      "88            0       0        0            0         0       0  14859.0   \n",
      "89            0       0        0            0         0       0   1754.0   \n",
      "90            0       0        0            0         0       0   2662.0   \n",
      "91            0       0        0            0         0       0   8535.0   \n",
      "92            0       0        0            0         0       0  22618.0   \n",
      "...         ...     ...      ...          ...       ...     ...      ...   \n",
      "2942730       1       0        0            0         0       0   3115.0   \n",
      "2942731       1       0        0            0         0       0    504.0   \n",
      "2942732       1       0        0            0         0       0    520.0   \n",
      "2942733       1       0        0            0         0       0  12678.0   \n",
      "2942734       1       0        0            0         0       0    346.0   \n",
      "\n",
      "         is_imputed  \n",
      "88                0  \n",
      "89                0  \n",
      "90                0  \n",
      "91                0  \n",
      "92                0  \n",
      "...             ...  \n",
      "2942730           0  \n",
      "2942731           0  \n",
      "2942732           0  \n",
      "2942733           0  \n",
      "2942734           0  \n",
      "\n",
      "[6203 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAPEs\n",
    "actuals = raw_imputed[raw_imputed['type_name'] == \"Demand\"]\n",
    "forecast = raw_imputed[raw_imputed['type_name'] == \"Day-ahead demand forecast\"]\n",
    "joined = pd.merge(actuals, forecast[['respondent', 'period', 'value']].rename(columns={'value': 'forecast'}),\n",
    "                  on=['respondent', 'period'], how='left')\n",
    "joined['abs_error'] = np.abs(joined['value'] - joined['forecast']) / np.abs(joined['value'])\n",
    "\n",
    "MAPE = joined[joined['abs_error'] != np.inf].groupby('respondent')['abs_error'].mean().reset_index(name='MAPE')\n",
    "compress_to_zip(\"MAPE\",MAPE)\n",
    "\n",
    "# Load edges and calculate correlations\n",
    "edges = extract_zip(\"eia_930_edges\")\n",
    "exclude = [\"CISO\", \"ERCO\", \"SWPP\", \"MISO\", \"NYIS\", \"ISNE\", \"CAL\", \"PJM\"]\n",
    "\n",
    "edges = edges.merge(MAPE, left_on=\"node1\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node1\"})\n",
    "edges = edges.merge(MAPE, left_on=\"node2\", right_on=\"respondent\").rename(columns={\"MAPE\": \"MAPE_node2\"})\n",
    "edges['abs_diff'] = np.abs(edges['MAPE_node1'] - edges['MAPE_node2'])\n",
    "edges = edges.query(\"~node1.isin(@exclude) & ~node2.isin(@exclude)\").sort_values('abs_diff', ascending=False)\n",
    "compress_to_zip(\"edges_with_MAPE\",edges)\n",
    "print(edges)\n",
    "\n",
    "# Wide format and correlation matrix\n",
    "duplicates = actuals[actuals.duplicated(subset=['period', 'respondent'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates found in actuals before pivot:\")\n",
    "    print(duplicates)\n",
    "    # dedup\n",
    "    actuals = actuals.drop_duplicates(subset=['period', 'respondent'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92b97878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respondent             AECI     AVA    AZPS    BANC    BPAT      CAL      CAR  \\\n",
      "period                                                                          \n",
      "2022-01-01 00:00:00  2599.0  1980.0  3141.0  1871.0  8535.0  27730.0  23263.0   \n",
      "2022-01-01 01:00:00  2581.0  2086.0  3328.0  1982.0  8977.0  29970.0  22455.0   \n",
      "2022-01-01 02:00:00  2511.0  2135.0  3409.0  2156.0  9407.0  33002.0  21491.0   \n",
      "2022-01-01 03:00:00  2441.0  2097.0  3317.0  2154.0  9369.0  33101.0  20686.0   \n",
      "2022-01-01 04:00:00  2382.0  2040.0  3248.0  2105.0  9183.0  32336.0  19643.0   \n",
      "...                     ...     ...     ...     ...     ...      ...      ...   \n",
      "2024-07-07 20:00:00  3213.0  1715.0  5985.0  3366.0  7539.0  34199.0  36618.0   \n",
      "2024-07-07 21:00:00  3320.0  1809.0  6533.0  3764.0  7856.0  35142.0  36594.0   \n",
      "2024-07-07 22:00:00  3420.0  1864.0  6912.0  3945.0  7444.0  36463.0  36177.0   \n",
      "2024-07-07 23:00:00  3448.0  1935.0  7142.0  4142.0  8294.0  38081.0  34997.0   \n",
      "2024-07-08 00:00:00  3351.0  1979.0  7554.0  4330.0  7789.0  40112.0  33750.0   \n",
      "\n",
      "respondent              CENT   CHPD     CISO  ...       TEN     TEPC      TEX  \\\n",
      "period                                        ...                               \n",
      "2022-01-01 00:00:00  30567.0  392.0  22618.0  ...  16554.00  1361.00  45943.0   \n",
      "2022-01-01 01:00:00  30805.0  427.0  24545.0  ...  16213.00  1363.00  46427.0   \n",
      "2022-01-01 02:00:00  30154.0  438.0  27081.0  ...  15715.00  1361.00  45058.0   \n",
      "2022-01-01 03:00:00  29574.0  433.0  27153.0  ...  15174.00  1327.00  43430.0   \n",
      "2022-01-01 04:00:00  28934.0  429.0  26522.0  ...  14683.00  1293.00  42058.0   \n",
      "...                      ...    ...      ...  ...       ...      ...      ...   \n",
      "2024-07-07 20:00:00  38311.0  236.0  29304.0  ...  26990.62  2469.91  67989.0   \n",
      "2024-07-07 21:00:00  38955.0  246.0  29752.0  ...  27482.36  2664.80  69434.0   \n",
      "2024-07-07 22:00:00  39582.0  256.0  30807.0  ...  27873.66  2805.73  70636.0   \n",
      "2024-07-07 23:00:00  39938.0  260.0  32146.0  ...  27749.80  2958.90  70127.0   \n",
      "2024-07-08 00:00:00  39468.0  263.0  33932.0  ...  26893.78  3077.85  68624.0   \n",
      "\n",
      "respondent            TIDC   TPWR       TVA       US48    WACM   WALC   WAUW  \n",
      "period                                                                        \n",
      "2022-01-01 00:00:00  277.0  807.0  16554.00  459742.00  2976.0  786.0  137.0  \n",
      "2022-01-01 01:00:00  290.0  845.0  16213.00  457338.00  3070.0  825.0  143.0  \n",
      "2022-01-01 02:00:00  308.0  882.0  15715.00  449030.00  2999.0  842.0  143.0  \n",
      "2022-01-01 03:00:00  306.0  885.0  15174.00  436049.00  2955.0  822.0  141.0  \n",
      "2022-01-01 04:00:00  296.0  862.0  14683.00  421312.00  2914.0  805.0  139.0  \n",
      "...                    ...    ...       ...        ...     ...    ...    ...  \n",
      "2024-07-07 20:00:00  526.0  531.0  26990.62  630963.53     NaN    NaN   91.0  \n",
      "2024-07-07 21:00:00  556.0  550.0  27482.36  646354.16     NaN    NaN   96.0  \n",
      "2024-07-07 22:00:00  585.0  566.0  27873.66  658244.39     NaN    NaN  111.0  \n",
      "2024-07-07 23:00:00  607.0  577.0  27749.80  661162.70     NaN    NaN  108.0  \n",
      "2024-07-08 00:00:00  622.0  598.0  26893.78  653511.63     NaN    NaN  113.0  \n",
      "\n",
      "[22056 rows x 67 columns]\n",
      "DataFrame saved and compressed into: ./data/correlation_matrix.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform pivot operation\n",
    "actuals_wide = actuals.pivot(index='period', columns='respondent', values='imputed')\n",
    "print(actuals_wide)\n",
    "correlation_matrix = actuals_wide.corr(method='pearson', min_periods=1)\n",
    "compress_to_zip(\"correlation_matrix\",correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2280c731",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Monday' 'Tuesday' 'Wednesday' 'Thursday' 'Friday' 'Sunday'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Simple LDWP Model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m relevant_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCISO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBPAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDWP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPACE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEVP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZPS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWALC\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuesday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWednesday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThursday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFriday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSunday\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m reg_data \u001b[38;5;241m=\u001b[39m \u001b[43mactuals_wide\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelevant_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      4\u001b[0m reg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDWP_lag1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDWP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m reg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDWP_lag24\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDWP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m24\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/multi.py:2536\u001b[0m, in \u001b[0;36mMultiIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keyarr) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keyarr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   2534\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_indexer_level_0(keyarr)\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[indexer], indexer\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/multi.py:2554\u001b[0m, in \u001b[0;36mMultiIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   2552\u001b[0m cmask \u001b[38;5;241m=\u001b[39m check \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m-> 2554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyarr[cmask]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;66;03m# We get here when levels still contain values which are not\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;66;03m# actually in Index anymore\u001b[39;00m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyarr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Monday' 'Tuesday' 'Wednesday' 'Thursday' 'Friday' 'Sunday'] not in index\""
     ]
    }
   ],
   "source": [
    "# Simple LDWP Model\n",
    "relevant_cols = ['CISO', 'BPAT', 'LDWP', 'PACE', 'NEVP', 'AZPS', 'WALC','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday']\n",
    "reg_data = actuals_wide[relevant_cols].dropna()\n",
    "reg_data['LDWP_lag1'] = reg_data['LDWP'].shift(1)\n",
    "reg_data['LDWP_lag24'] = reg_data['LDWP'].shift(24)\n",
    "\n",
    "reg_data = reg_data.dropna()\n",
    "X = reg_data[['LDWP_lag1', 'LDWP_lag24', 'CISO', 'BPAT', 'PACE', 'NEVP', 'AZPS', 'WALC', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday']]\n",
    "y = reg_data['LDWP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb766db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter grids for GridSearchCV\n",
    "linear_param_grid = {\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, shuffle=True, random_state=614\n",
    "        )\n",
    "\n",
    "# Model 1: Linear Regression with GridSearchCV and Cross-Validation\n",
    "linear_model = LinearRegression()\n",
    "linear_grid_search = GridSearchCV(linear_model, linear_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Linear Regression\n",
    "linear_cv_results = cross_validate(linear_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "linear_mean_score = linear_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Linear Regression Model\n",
    "best_linear_model = linear_grid_search.best_estimator_\n",
    "linear_predictions = best_linear_model.predict(X_test)\n",
    "linear_mape = mean_absolute_percentage_error(y_test, linear_predictions)\n",
    "with open(\"./models/linear_regression_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_linear_model, f)\n",
    "print(\"Best Linear Regression Model saved successfully.\")\n",
    "\n",
    "# Model 2: Random Forest Regressor with GridSearchCV and Cross-Validation\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "rf_cv_results = cross_validate(rf_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "rf_mean_score = rf_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Random Forest Model\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "rf_mape = mean_absolute_percentage_error(y_test, rf_predictions)\n",
    "with open(\"./models/random_forest_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_rf_model, f)\n",
    "print(\"Best Random Forest Model saved successfully.\")\n",
    "\n",
    "# Model 3: Gradient Boosting Regressor with GridSearchCV and Cross-Validation\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_grid_search = GridSearchCV(gb_model, gb_param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for Gradient Boosting\n",
    "gb_cv_results = cross_validate(gb_grid_search.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "gb_mean_score = gb_cv_results['test_score'].mean()\n",
    "\n",
    "# Best Gradient Boosting Model\n",
    "best_gb_model = gb_grid_search.best_estimator_\n",
    "gb_predictions = best_gb_model.predict(X_test)\n",
    "gb_mape = mean_absolute_percentage_error(y_test, gb_predictions)\n",
    "with open(\"./models/gradient_boosting_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_gb_model, f)\n",
    "print(\"Best Gradient Boosting Model saved successfully.\")\n",
    "\n",
    "# Compare model accuracy\n",
    "print(\"Model Performance (MAPE):\")\n",
    "print(f\"Linear Regression: {linear_mape:.4f}, Cross-Validation Mean: {linear_mean_score:.4f}\")\n",
    "print(f\"Random Forest Regressor: {rf_mape:.4f}, Cross-Validation Mean: {rf_mean_score:.4f}\")\n",
    "print(f\"Gradient Boosting Regressor: {gb_mape:.4f}, Cross-Validation Mean: {gb_mean_score:.4f}\")\n",
    "\n",
    "# Evaluation results dictionary\n",
    "evaluation_results = {\n",
    "    \"Linear Regression\": {\"MAPE\": linear_mape, \"Cross-Validation Mean\": linear_mean_score},\n",
    "    \"Random Forest\": {\"MAPE\": rf_mape, \"Cross-Validation Mean\": rf_mean_score},\n",
    "    \"Gradient Boosting\": {\"MAPE\": gb_mape, \"Cross-Validation Mean\": gb_mean_score},\n",
    "}\n",
    "\n",
    "# Save evaluation results to JSON\n",
    "with open(\"./data/evaluation_results.json\", \"w\") as file:\n",
    "    json.dump(evaluation_results, file)\n",
    "\n",
    "print(\"Evaluation results saved successfully.\")\n",
    "\n",
    "# Print model coefficients or feature importances\n",
    "print(\"Linear Model Coefficients:\", best_linear_model.coef_)\n",
    "print(\"Random Forest Feature Importances:\", best_rf_model.feature_importances_)\n",
    "print(\"Gradient Boosting Feature Importances:\", best_gb_model.feature_importances_)\n",
    "\n",
    "print(\"Results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8916fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to: ./data/edges_with_MAPE.csv\n",
      "    node1 node2 respondent_x  MAPE_node1 respondent_y  MAPE_node2  abs_diff\n",
      "0    AECI   SPA         AECI    0.036209          SPA    1.044349  1.008140\n",
      "1     SPA  AECI          SPA    1.044349         AECI    0.036209  1.008140\n",
      "2    PSCO   PNM         PSCO    0.723519          PNM    0.063648  0.659871\n",
      "3     PNM  PSCO          PNM    0.063648         PSCO    0.723519  0.659871\n",
      "4    PSCO  WACM         PSCO    0.723519         WACM    0.142075  0.581444\n",
      "..    ...   ...          ...         ...          ...         ...       ...\n",
      "187  GCPD  BPAT         GCPD    0.023107         BPAT    0.020857  0.002250\n",
      "188  BPAT   AVA         BPAT    0.020857          AVA    0.019132  0.001726\n",
      "189   AVA  BPAT          AVA    0.019132         BPAT    0.020857  0.001726\n",
      "190   TEC  FMPP          TEC    0.046723         FMPP    0.045600  0.001123\n",
      "191  FMPP   TEC         FMPP    0.045600          TEC    0.046723  0.001123\n",
      "\n",
      "[192 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "edges = extract_zip(\"edges_with_MAPE\")\n",
    "print(edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
